<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Meng Zheng</title>

  <meta name="author" content="Meng Zheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:68%;vertical-align:middle">
              <p style="text-align:center">
                <name>Meng Zheng</name>
              </p>
              <p>I am a senior research scientist at United Imaging Intelligence (UII), Cambridge MA.  
              </p>
              <p>
                I obtained my PhD degree from <a href="https://ecse.rpi.edu/">ECSE Department of Rensselaer Polytechnic Institute</a>. My advisor is <a href="https://sites.ecse.rpi.edu/~rjradke/">Prof. Rich Radke</a>. 
                I obtained my master and bachelor degree from Beijing Institue of Technology, Beijing, China.
              </p>
              <p style="text-align:center">
                <a href="mailto:mengzhengrpi@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV_Meng.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=1D5PfMgAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/meng-zheng-27ab35144/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/Meng.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Meng.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research focuses on the design and development of interpretable deep learning systems and algorithms for a variety of computer vision and medical imaging applications, with particular focus on automated patient positioning and modeling, video analytics, image retrieval/person re-identification and generative modeling. Representative papers are listed as follows.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <tr onmouseout="multivHMR_stop()" onmouseover="multivHMR_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/Multiv_HMR_AAAI23.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/abs/2212.05223">
              <papertitle>Progressive Multi-view Human Mesh Recovery with Self-Supervision</papertitle>
            </a>
            <br>
            <a href="https://gong-xuan.github.io/">Xuan Gong</a>, 
            <a href="https://lsongx.github.io/">Liangcheng Song</a>, 
            <strong>Meng Zheng</strong>, 
            <a href="https://planche.me/">Benjamin Planche</a>, 
            Terrence Chen, Junsong Yuan, David Doermann,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>AAAI (<strong>oral</strong>)</em>, 2023
            <p></p>
            <p>
              A novel simulation-based training pipeline for multi-view human mesh recovery.
            </p>
          </td>
        </tr>		

        <tr onmouseout="DPatientM_stop()" onmouseover="DPatientM_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/3DPatientM_MICCAI22.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://link.springer.com/chapter/10.1007/978-3-031-16449-1_12">
              <papertitle>Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion</papertitle>
            </a>
            <br>
            <strong>Meng Zheng</strong>, 
            <a href="https://planche.me/">Benjamin Planche</a>, 
            <a href="https://gong-xuan.github.io/">Xuan Gong</a>, 
            <a href="https://orcid.org/0000-0003-1535-447X">Fan Yang</a>, 
            Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>MICCAI (<strong>early accept</strong>)</em>, 2022
            <p></p>
            <p>
              A generic modularized 3D patient modeling method with generalized body keypoint detection and self-supervised mesh modeling.
            </p>
          </td>
        </tr>	

        <tr onmouseout="ASN_stop()" onmouseover="ASN_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/ASN_IJCAI22.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://www.ijcai.org/proceedings/2022/241">
              <papertitle>Visual Similarity Attention</papertitle>
            </a>
            <br>
            <strong>Meng Zheng</strong>, 
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>, 
            Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>,
            <a href="https://sites.ecse.rpi.edu/~rjradke/">Richard J. Radke</a>
            <br>
            <em>IJCAI</em>, 2022
            <p></p>
            <p>
              A novel method to generate generic visual similarity explanations with gradient-based attention.
            </p>
          </td>
        </tr>	

        <tr onmouseout="FL_TMI23_stop()" onmouseover="FL_TMI23_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/FL_TMI23.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/abs/2210.08464">
              <papertitle>Federated Learning with Privacy-Preserving Ensemble Attention Distillation</papertitle>
            </a>
            <br>
            <a href="https://gong-xuan.github.io/">Xuan Gong</a>, 
            <a href="https://lsongx.github.io/">Liangcheng Song</a>, 
            Rishi Vedula, Abhishek Sharma, <strong>Meng Zheng</strong>, 
            <a href="https://planche.me/">Benjamin Planche</a>,
            Arun Innanje, Terrence Chen, Junsong Yuan, David Doermann,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>IEEE Transactions on Medical Imaging</em>, 2023
            <p></p>
            <p>
              A privacy-preserving FL framework leveraging unlabeled public data for one-way offline knowledge distillation.
            </p>
          </td>
        </tr>	

        <tr onmouseout="CRAeccv_stop()" onmouseover="CRAeccv_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/CRA_ECCV22.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/abs/2209.04596">
              <papertitle>Self-supervised Human Mesh Recovery with Cross-Representation Alignment</papertitle>
            </a>
            <br>
            <a href="https://gong-xuan.github.io/">Xuan Gong</a>,
            <strong>Meng Zheng</strong>,
            <a href="https://planche.me/">Benjamin Planche</a>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>, 
            Terrence Chen, David Doermann,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>ECCV</em>, 2022
            <p></p>
            <p>
              Cross-representation alignment utilizing the complementary information from the robust but sparse representation (2D keypoints).
            </p>
          </td>
        </tr>	

        <tr onmouseout="Pseudoeccv_stop()" onmouseover="Pseudoeccv_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/PseudoClick_ECCV22.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660717.pdf">
              <papertitle>PseudoClick: Interactive Image Segmentation with Click Imitation</papertitle>
            </a>
            <br>
            <a href="https://sites.google.com/cs.unc.edu/qinliu/home?pli=1">Qin Liu</a>,
            <strong>Meng Zheng</strong>,
            <a href="https://planche.me/">Benjamin Planche</a>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>, 
            Terrence Chen, Marc Niethammer,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>ECCV</em>, 2022
            <p></p>
            <p>
              A generic framework that enables existing segmentation networks to propose candidate next clicks.
            </p>
          </td>
        </tr>	

        <tr onmouseout="PREFeccv_stop()" onmouseover="PREFeccv_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/PREF_ECCV22.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136820643.pdf">
              <papertitle>PREF: Predictability Regularized Neural Motion Fields</papertitle>
            </a>
            <br>
            <a href="https://lsongx.github.io/">Liangcheng Song</a>,
            <a href="https://gong-xuan.github.io/">Xuan Gong</a>, 
            <a href="https://planche.me/">Benjamin Planche</a>,
            <strong>Meng Zheng</strong>,
            David Doermann, Junsong Yuan, Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>ECCV (<strong>oral</strong>)</em>, 2022
            <p></p>
            <p>
              A predictability regularization by first conditioning the estimated motion on latent embeddings, then by adopting a predictor network to enforce predictability on the embeddings.
            </p>
          </td>
        </tr>	

        <tr onmouseout="SMPLA_stop()" onmouseover="SMPLA_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/SMPLA_CVPR22.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_SMPL-A_Modeling_Person-Specific_Deformable_Anatomy_CVPR_2022_paper.pdf">
              <papertitle>SMPL-A: Modeling Person-Specific Deformable Anatomy</papertitle>
            </a>
            <br>
            <a href="https://lsongx.github.io/">Hengtao Guo</a>,
            <a href="https://planche.me/">Benjamin Planche</a>,
            <strong>Meng Zheng</strong>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>, 
            Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>CVPR</em>, 2022
            <p></p>
            <p>
              A learning-based approach to estimate the patient's internal organ deformation for arbitrary human poses in order to assist with radiotherapy and similar medical protocols.
            </p>
          </td>
        </tr>	

        <tr onmouseout="VOS_WACV22_stop()" onmouseover="VOS_WACV22_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/VOS_WACV22.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Yang_Multi-Motion_and_Appearance_Self-Supervised_Moving_Object_Detection_WACV_2022_paper.pdf">
              <papertitle>Multi-motion and Appearance Self-Supervised Moving Object Detection</papertitle>
            </a>
            <br>
            <a href="https://orcid.org/0000-0003-1535-447X">Fan Yang</a>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
            <strong>Meng Zheng</strong>,
            Terrence Chen, Haibin Ling,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>WACV</em>, 2022
            <p></p>
            <p>
              A Multimotion and Appearance Self-supervised Network (MASNet) to introduce multi-scale motion information and appearance information of scene for MOD.
            </p>
          </td>
        </tr>	


        <tr onmouseout="HMR_BMVC21_stop()" onmouseover="HMR_BMVC21_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/HMR_BMVC21.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://www.bmvc2021-virtualconference.com/assets/papers/0487.pdf">
              <papertitle>Everybody Is Unique: Towards Unbiased Human Mesh Recovery</papertitle>
            </a>
            <br>
            <a href="https://liren2515.github.io/page/">Ren Li</a>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
            <strong>Meng Zheng</strong>,
            Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>BMVC (<strong>oral</strong>)</em>, 2021
            <p></p>
            <p>
              A solution to the problem of obese human mesh recovery, i.e., fitting a parametric human mesh to images of obese people.
            </p>
          </td>
        </tr>	

        <tr onmouseout="STRF_ICCV21_stop()" onmouseover="STRF_ICCV21_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/STRF_ICCV21.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Aich_Spatio-Temporal_Representation_Factorization_for_Video-Based_Person_Re-Identification_ICCV_2021_paper.pdf">
              <papertitle>Spatio-Temporal Representation Factorization for Video-based Person Re-Identification</papertitle>
            </a>
            <br>
            <a href="https://abhishekaich27.github.io/">Abhishek Aich</a>,
            <strong>Meng Zheng</strong>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
            Amit K. Roy-Chowdhury, Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>ICCV, 2021
            <p></p>
            <p>
              Explicit pathways for learning discriminative temporal and spatial features, with each component further factorized to capture complementary person-specific appearance and motion information.
            </p>
          </td>
        </tr>	

        <tr onmouseout="VRX_CVPR21_stop()" onmouseover="VRX_CVPR21_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/VRX_CVPR21.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_A_Peek_Into_the_Reasoning_of_Neural_Networks_Interpreting_With_CVPR_2021_paper.pdf">
              <papertitle>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</papertitle>
            </a>
            <br>
            <a href="https://gyhandy.github.io/">Yunhao Ge</a>,
            Yao Xiao, Zhi Xu,
            <strong>Meng Zheng</strong>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
            Terrence Chen, Laurent Itti,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>CVPR, 2021
            <p></p>
            <p>
              A framework (VRX) to interpret classification NNs with intuitive structural visual concepts.
            </p>
          </td>
        </tr>	

        <tr onmouseout="EVAE_CVPR20_stop()" onmouseover="EVAE_CVPR20_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/EVAE_CVPR2020.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Towards_Visually_Explaining_Variational_Autoencoders_CVPR_2020_paper.pdf">
              <papertitle>Towards Visually Explaining Variational Autoencoders</papertitle>
            </a>
            <br>
            <a href="https://scholar.google.com/citations?user=6yh4f4YAAAAJ&hl=en">Wenqian Liu*</a>,
            <a href="http://bragilee.github.io/">Runze Li*</a>,
            <strong>Meng Zheng</strong>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
            <a href="http://wuziyan.com/">Ziyan Wu</a>,
            Bir Bhanu, <a href="https://sites.ecse.rpi.edu/~rjradke/">Richard J. Radke</a>,
            Octavia Camps
            <br>
            <em>CVPR, 2020
            <p></p>
            <p>
              A novel technique to visually explain VAEs by means of gradient-based attention.
            </p>
          </td>
        </tr>	

        <tr onmouseout="CASN_CVPR19_stop()" onmouseover="CASN_CVPR19_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/CASN_CVPR19.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Re-Identification_With_Consistent_Attentive_Siamese_Networks_CVPR_2019_paper.pdf">
              <papertitle>Re-Identification with Consistent Attentive Siamese Networks</papertitle>
            </a>
            <br>
            <strong>Meng Zheng</strong>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
            <a href="http://wuziyan.com/">Ziyan Wu</a>,
            <a href="https://sites.ecse.rpi.edu/~rjradke/">Richard J. Radke</a>
            <br>
            <em>CVPR, 2019
            <p></p>
            <p>
              A framework for re-id that provides mechanisms to make attention and attention consistency end-to-end trainable in a Siamese learning architecture, resulting in a technique for robust cross-view matching as well as explaining the reasoning for why the model predicts that the two images belong to the same person
            </p>
          </td>
        </tr>	

        <tr onmouseout="RPIfield_stop()" onmouseover="RPIfield_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/RPIfield_CVPRW18.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w36/Zheng_RPIfield_A_New_CVPR_2018_paper.pdf">
              <papertitle>RPIfield: A New Dataset for Temporally Evaluating Person Re-identification</papertitle>
            </a>
            <br>
            <strong>Meng Zheng</strong>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
            <a href="https://sites.ecse.rpi.edu/~rjradke/">Richard J. Radke</a>
            <br>
            <em>CVPR Workshop, 2018
            <p></p>
            <p>
              A new multi-shot re-id dataset, called RPIfield, which provides explicit time-stamp information for each candidate.
            </p>
          </td>
        </tr>	

        <tr onmouseout="TBIOM20_stop()" onmouseover="TBIOM20_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/TBIOM_20.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://ieeexplore.ieee.org/document/9186207">
              <papertitle>Towards Automated Spatio-Temporal Trajectory Recovery in Wide-Area Camera Networks</papertitle>
            </a>
            <br>
            <strong>Meng Zheng</strong>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
            <a href="https://sites.ecse.rpi.edu/~rjradke/">Richard J. Radke</a>
            <br>
            <em>IEEE Transactions on Biometrics, Behavior, and Identity Science, 2020
            <p></p>
            <p>
              A new algorithm to automatically reconstruct the time-stamped spatial trajectory of a person of interest moving in a camera network.
            </p>
          </td>
        </tr>		

        <tr onmouseout="SPM18_stop()" onmouseover="SPM18_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/SPM18.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://ieeexplore.ieee.org/document/8253598">
              <papertitle>The Deep Regression Bayesian Network and Its Applications: Probabilistic Deep Learning for Computer Vision</papertitle>
            </a>
            <br>
            <a href="https://scholar.google.com/citations?user=PgbUI2sAAAAJ&hl=en">Siqi Nie</a>,
            <strong>Meng Zheng</strong>, Qiang Ji
            <br>
            <em>IEEE Signal Processing Magazine, 2018
            <p></p>
            <p>
              A review of different structures of deep directed generative models and the learning and inference algorithms associated with the structures.
            </p>
          </td>
        </tr>	

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Review Activities</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

        <tr>
          <td width="100%" valign="center">
            IEEE Signal Processing Letters
            <br>
            IEEE Transactions on Neural Networks and Learning Systems
            <br>
            IEEE Transactions on Circuits and Systems for Video Technology
            <br>
            IEEE Access
            <br>
            European Conference on Computer Vision (ECCV)
            <br>
            IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
            <br>
            IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
            <br>         
          </td>
        </tr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

        <tr>
          <td width="100%" valign="center">
            <a href="https://alert.northeastern.edu/news-article/from-phd-candidates-to-professionals-alert-trio-continues-to-collaborate/">Interesting and exciting media post about my advisor and collaborators!</a>
            <br>         
          </td>
        </tr>


        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:x-small;">
                Thanks to Jon Barron for sharing <a href="https://github.com/jonbarron/jonbarron_website">source code</a> for creating this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
