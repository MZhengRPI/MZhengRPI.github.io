<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Meng Zheng</title>

  <meta name="author" content="Meng Zheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:68%;vertical-align:middle">
              <p style="text-align:center">
                <name>Meng Zheng</name>
              </p>
              <p>I am an expert research scientist at United Imaging Intelligence (UII), Burlington MA.  
              </p>
              <p>
                I obtained my PhD degree from <a href="https://ecse.rpi.edu/">ECSE Department of Rensselaer Polytechnic Institute</a>. My advisor is <a href="https://sites.ecse.rpi.edu/~rjradke/">Prof. Rich Radke</a>. 
                I received my master and bachelor degree from Beijing Institue of Technology, Beijing, China.
              </p>
              <p style="text-align:center">
                <a href="mailto:mengzhengrpi@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV_Meng.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=1D5PfMgAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/meng-zheng-27ab35144/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/Meng.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Meng.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research focuses on the design and development of interpretable deep learning systems and algorithms for a variety of computer vision and medical imaging applications, with particular focus on automated patient positioning and modeling, video analytics, image retrieval/person re-identification and generative modeling. Representative papers are listed as follows.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr onmouseout="chrome_stop()" onmouseover="chrome_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/chrome_ICCV25_1.png' width="190">
              <br>
              <br>
              <img src='images/chrome_ICCV25_2.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/pdf/2503.15671">
              <papertitle>Chrome: Clothed human reconstruction with occlusion-resilience and multiview-consistency from a single image</papertitle>
            </a>
            <br>
            Arindam Dutta, 
            <strong>Meng Zheng</strong>, 
            <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, 
            <a href="https://planche.me/">Benjamin Planche</a>, 
            Anwesha Choudhuri, Terrence Chen, Amit K Roy-Chowdhury, 
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>ICCV (<strong>highlight</strong>)</em>, 2025
            <p></p>
            <p>
              We propose CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-ConsistEncy from a Single Image, a novel pipeline designed to reconstruct occlusion-resilient 3D humans with multiview consistency from a single occluded image, without requiring either ground-truth geometric prior annotations or 3D supervision. CHROME leverages a multiview diffusion model to first synthesize occlusion-free human images from the occluded input, compatible with off-the-shelf pose control to explicitly enforce cross-view consistency during synthesis. A 3D reconstruction model is then trained to predict a set of 3D Gaussians conditioned on both the occluded input and synthesized views, aligning cross-view details to produce a cohesive and accurate 3D representation. 
            </p>
          </td>
        </tr>	


        <tr onmouseout="sevenDGS_stop()" onmouseover="sevenDGS_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/7DGS_ICCV25_1.png' width="190">
              <br>
              <br>
              <img src='images/7DGS_ICCV25_2.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/pdf/2503.07946">
              <papertitle>7DGS: Unified Spatial-Temporal-Angular Gaussian Splatting</papertitle>
            </a>
            <br>
            <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, 
            <a href="https://planche.me/">Benjamin Planche</a>, 
            <strong>Meng Zheng</strong>, 
            Anwesha Choudhuri, Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>ICCV</em>, 2025
            <p></p>
            <p>
              We present 7D Gaussian Splatting (7DGS), a unified framework representing scene elements as seven-dimensional Gaussians spanning position (3D), time (1D), and viewing direction (3D). Our key contribution is an efficient conditional slicing mechanism that transforms 7D Gaussians into view- and time-conditioned 3D Gaussians, maintaining compatibility with existing 3D Gaussian Splatting pipelines while enabling joint optimization. Experiments demonstrate that 7DGS outperforms prior methods by up to 7.36 dB in PSNR while achieving real-time rendering (401 FPS) on challenging dynamic scenes with complex view-dependent effects.
            </p>
          </td>
        </tr>	


        <tr onmouseout="segtrack_stop()" onmouseover="segtrack_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/SegTrack_MICCAI25.png' width="190">
              <br>
              <br>
              <img src='images/SegTrack_MICCAI25_2.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/abs/2503.24108">
              <papertitle>PolypSegTrack: Unified Foundation Model for Colonoscopy Video Analysis</papertitle>
            </a>
            <br>
            Anwesha Choudhuri, 
            <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, 
            <strong>Meng Zheng</strong>, 
            <a href="https://planche.me/">Benjamin Planche</a>, 
            Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>MICCAI</em>, 2025
            <p></p>
            <p>
              We introduce PolypSegTrack, a novel foundation model that jointly addresses polyp detection, segmentation, classification and unsupervised tracking in colonoscopic videos. Our approach leverages a novel conditional mask loss, enabling flexible training across datasets with either pixel-level segmentation masks or bounding box annotations, allowing us to bypass task-specific fine-tuning. Our unsupervised tracking module reliably associates polyp instances across frames using object queries, without relying on any heuristics. We leverage a robust vision foundation model backbone that is pre-trained unsupervisedly on natural images, thereby removing the need for domain-specific pre-training. 
            </p>
          </td>
        </tr>	
        
        
        <tr onmouseout="seq2time_stop()" onmouseover="seq2time_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/seq2time_CVPR25.png' width="190">
              <br>
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/pdf/2411.16932">
              <papertitle>PolypSegTrack: Unified Foundation Model for Colonoscopy Video Analysis</papertitle>
            </a>
            <br>
            Andong Deng, 
            <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, 
            <a href="https://scholar.google.com/citations?user=KQnKUHKGFdUC&hl=en">Anwesa Choudhuri</a>, 
            <a href="https://planche.me/">Benjamin Planche</a>, 
            <strong>Meng Zheng</strong>, 
            Bin Wang, Chen Chen, 
            Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>CVPR</em>, 2025
            <p></p>
            <p>
              We propose Seq2Time, a data-oriented training paradigm that leverages sequences of images and short video clips to enhance temporal awareness in long videos. By converting sequence positions into temporal annotations, we transform largescale image and clip captioning datasets into sequences that mimic the temporal structure of long videos, enabling self-supervised training with abundant time-sensitive data. To enable sequence-to-time knowledge transfer, we introduce a novel time representation that unifies positional information across image sequences, clip sequences, and long videos.
            </p>
          </td>
        </tr>	


        <tr onmouseout="threeDFSS_stop()" onmouseover="threeDFSS_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/3DFSS_1_MICCAI24.png' width="190">
              <br>
              <br>
              <img src='images/3DFSS_2_MICCAI24.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/abs/2408.14427">
              <papertitle>Few-Shot 3D Volumetric Segmentation with Multi-Surrogate Fusion</papertitle>
            </a>
            <br>
            <strong>Meng Zheng</strong>, 
            <a href="https://planche.me/">Benjamin Planche</a>, 
            <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>,
            Terrence Chen,
            <a href="https://sites.ecse.rpi.edu/~rjradke/">Richard J. Radke</a>,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>MICCAI (<strong>early accept</strong>)</em>, 2024
            <p></p>
            <p>
              We present MSFSeg, a novel few-shot 3D segmentation framework with a lightweight multi-surrogate fusion (MSF). MSFSeg is able to automatically segment unseen 3D objects/organs (during training) provided with one or a few annotated 2D slices or 3D sequence segments, via learning dense query-support organ/lesion anatomy correlations across patient populations. Our proposed MSF module mines comprehensive and diversified morphology correlations between unlabeled and the few labeled slices/sequences through multiple designated surrogates, making it able to generate accurate cross-domain 3D segmentation masks given annotated slices or sequences. We demonstrate the effectiveness of our proposed framework by showing superior performance on conventional few-shot segmentation benchmarks compared to prior art, and remarkable cross-domain cross-volume segmentation performance on proprietary 3D segmentation datasets for challenging entities, i.e., tubular structures, with only limited 2D or 3D labels.
            </p>
          </td>
        </tr>	

        
        <tr onmouseout="DDGS_stop()" onmouseover="DDGS_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/DDGS_ICLR24_1.png' width="190">
              <br>
              <br>
              <img src='images/DDGS_ICLR24_2.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/456ce0476c9b4689a74918b851cecd5a-Paper-Conference.pdf">
              <papertitle>DDGS-CT: Direction-disentangled gaussian splatting for realistic volume rendering</papertitle>
            </a>
            <br>
            <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, <a href="https://planche.me/">Benjamin Planche</a>, 
            <strong>Meng Zheng</strong>, Xiao Chen, Terrence Chen, Ziyan Wu
            <br>
            <em>NeurIPS</em>, 2024
            <p></p>
            <p>
              We propose a novel approach that balances realistic physics-inspired X-ray simulation with efficient, differentiable DRR generation using 3D Gaussian splatting (3DGS). Our direction-disentangled 3DGS (DDGS) method decomposes the radiosity contribution into isotropic and direction-dependent components, able to approximate complex anisotropic interactions without complex runtime simulations. Additionally, we adapt the 3DGS initialization to account for tomography data properties, enhancing accuracy and efficiency.
            </p>
          </td>
        </tr>	


        <tr onmouseout="gestureWACV25_stop()" onmouseover="gestureWACV25_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/gesture_WACV_25.png' width="190">
              <br>
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/pdf/2407.14903">
              <papertitle>Automated Patient Positioning with Learned 3D Hand Gestures</papertitle>
            </a>
            <br>
            <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>*, Abhishek Sharma, 
            <strong>Meng Zheng</strong>, 
            <a href="https://planche.me/">Benjamin Planche</a>, 
            Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>WACV</em>, 2025
            <p></p>
            <p>
              We propose an automated patient positioning system that utilizes a camera to detect specific hand gestures from technicians, allowing users to indicate the target patient region to the system and initiate automated positioning. Our approach relies on a novel multi-stage pipeline to recognize and interpret the technicians‚Äô gestures, translating them into precise motions of medical devices.
            </p>
          </td>
        </tr>	


        <tr onmouseout="OIS_VLM_stop()" onmouseover="OIS_VLM_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/OIS_ICLR24.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/abs/2410.12214">
              <papertitle>Order-aware Interactive Segmentation</papertitle>
            </a>
            <br>
            Bin Wang, 
            <a href="https://scholar.google.com/citations?user=KQnKUHKGFdUC&hl=en">Anwesa Choudhuri</a>,
            <strong>Meng Zheng</strong>, 
            <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, 
            <a href="https://planche.me/">Benjamin Planche</a>, Andong Deng, Qin Liu, Terrence Chen, Ulas Bagci, Ziyan Wu
            <br>
            <em>ICLR</em>, 2025
            <p></p>
            <p>
              We propose OIS: order-aware interactive segmentation, where we explicitly encode the relative depth between objects into order maps. We introduce a novel order-aware attention, where the order maps seamlessly guide the user interactions (in the form of clicks) to attend to the image features. We further present an object-aware attention module to incorporate a strong object-level understanding to better differentiate objects with similar order. Our approach allows both dense and sparse integration of user clicks, enhancing both accuracy and efficiency as compared to prior works.
            </p>
          </td>
        </tr>	

        <tr onmouseout="sDGS_VLM_stop()" onmouseover="sDGS_VLM_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/6DGS_ICLR24.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/pdf/2410.04974">
              <papertitle>6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric Rendering</papertitle>
            </a>
            <br>
            <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, <a href="https://planche.me/">Benjamin Planche</a>, 
            <strong>Meng Zheng</strong>, 
            <a href="https://scholar.google.com/citations?user=KQnKUHKGFdUC&hl=en">Anwesa Choudhuri</a>, Terrence Chen, Ziyan Wu
            <br>
            <em>ICLR</em>, 2025
            <p></p>
            <p>
              We revisit 6D Gaussians and introduce 6D Gaussian Splatting (6DGS), which enhances color and opacity representations and leverages the additional directional information in the 6D space for optimized Gaussian control. Our approach is fully compatible with the 3DGS framework and significantly improves real-time radiance field rendering by better modeling view-dependent effects and fine details. The project page is: https://gaozhongpai.github.io/6dgs/
            </p>
          </td>
        </tr>	


        <tr onmouseout="tDVLGS_stop()" onmouseover="tDVLGS_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/3DVLGS_ICLR24.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/pdf/2410.07577">
              <papertitle>3D Vision-Language Gaussian Splatting</papertitle>
            </a>
            <br>
            Qucheng Peng, <a href="https://planche.me/">Benjamin Planche</a>, 
            <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, 
            <strong>Meng Zheng</strong>, 
            <a href="https://scholar.google.com/citations?user=KQnKUHKGFdUC&hl=en">Anwesa Choudhuri</a>, Terrence Chen, Chen Chen, Ziyan Wu
            <br>
            <em>ICLR</em>, 2025
            <p></p>
            <p>
              We propose a solution that adequately handles the distinct visual and semantic modalities, i.e., a 3D vision-language Gaussian splatting model for scene understanding, to put emphasis on the representation learning of language modality. We propose a novel cross-modal rasterizer, using modality fusion along with a smoothed semantic indicator for enhancing semantic rasterization. We also employ a camera-view blending technique to improve semantic consistency between existing and synthesized views, thereby effectively mitigating over-fitting.
            </p>
          </td>
        </tr>	

        
        <tr onmouseout="CCDA_VLM_stop()" onmouseover="CCDA_VLM_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/CCDA_VLM_MM.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://openreview.net/forum?id=TjFn6xktTm">
              <papertitle>Cross-Class Domain Adaptive Semantic Segmentation with Visual Language Models</papertitle>
            </a>
            <br>
            Wenqi Ren, 
            Ruihao Xia,
            <strong>Meng Zheng</strong>, 
            <a href="http://wuziyan.com/">Ziyan Wu</a>,
            Yang Tang, 
            Nicu Sebe
            <br>
            <em>ACM MM</em>, 2024
            <p></p>
            <p>
              We propose a label alignment method by leveraging VLMs to relabel pseudo labels for novel classes. Considering that VLMs typically provide only image-level predictions, we embed a two-stage method to enable fine-grained semantic segmentation and design a threshold based on the uncertainty of pseudo labels to exclude noisy VLM predictions. To further augment the supervision of novel classes, we devise memory banks with an adaptive update scheme to effectively manage accurate VLM predictions, which are then resampled to increase the sampling probability of novel classes. Through comprehensive experiments, we demonstrate the effectiveness and versatility of our proposed method across various CCDA scenarios.
            </p>
          </td>
        </tr>	

        <tr onmouseout="DF_HMR_stop()" onmouseover="DF_HMR_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/DF_HMR_ECCV24_1.png' width="190">
              <br>
              <br>
              <img src='images/DF_HMR_ECCV24_2.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/pdf/2407.09694">
              <papertitle>Divide and Fuse: Body Part Mesh Recovery from Partially Visible Human Images</papertitle>
            </a>
            <br>
            Tianyu Luan, <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, Luyuan Xie, Abhishek Sharma, Hao Ding, 
            <a href="https://planche.me/">Benjamin Planche</a>, 
            <strong>Meng Zheng</strong>, 
            Ange Lou, Terrence Chen, 
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>ECCV</em>, 2024
            <p></p>
            <p>
              We introduce a novel bottom-up approach for human body mesh reconstruction, specifically designed to address the challenges posed by partial visibility and occlusion in input images. 
              Traditional top-down methods, relying on whole-body parametric models like SMPL, falter when only a small part of the human is visible, as they require visibility of most of the human body for accurate mesh reconstruction. 
              To overcome this limitation, our method employs a "Divide and Fuse (D&F)" strategy, reconstructing human body parts independently before fusing them, thereby ensuring robustness against occlusions. 
              We design Human Part Parametric Models (HPPM) that independently reconstruct the mesh from a few shape and global location parameters, without inter-part dependency. 
              A specially designed fusion module then seamlessly integrates the reconstructed parts, even when only a few parts are visible. 
            </p>
          </td>
        </tr>	

        <tr onmouseout="SMPLA_AAAI24_stop()" onmouseover="SMPLA_AAAI24_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img class="image1" src="images/SMPLA_AAAI24_pip.png" width="190" />
              <br>  
              <br>  
              <img class="image2" src="images/SMPLA_AAAI24_vis.png" width="190"/>
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
	        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28171">
          	<papertitle>Implicit Modeling of Non-rigid Objects with Cross-Category Signals</papertitle>
	          </a>
            <br>
            Yuchun Liu, 
            <a href="https://planche.me/">Benjamin Planche</a>,
            <strong>Meng Zheng</strong>, 
            <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>,
            Pierre Sibut-Bourde,
            Fan Yang,
            Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>,
            <br>
            <em>AAAI</em>, 2024
            <p></p>
            <p>
              We propose MODIF, a multi-object deep implicit function that jointly learns the deformation fields and instance-specific latent codes for multiple objects at once. Our emphasis is on non-rigid, non-interpenetrating entities such as organs. To effectively capture the interrelation between these entities and ensure precise, collision-free representations, our approach facilitates signaling between category-specific fields to adequately rectify shapes. We also introduce novel inter-object supervision: an attraction-repulsion loss is formulated to refine contact regions between objects. Our approach is demonstrated on various medical benchmarks, involving modeling different groups of intricate anatomical entities.
            </p>
          </td>
        </tr>	

        <tr onmouseout="FaceDeID_AAAI24_stop()" onmouseover="FaceDeID_AAAI24_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src="images/FaceDeID_AAAI24.png" width="190" />
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
	    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27851">
          	<papertitle>Disguise without Disruption: Utility-Preserving Face De-identification</papertitle>
	    </a>
            <br>
            Zikui Cai, 
            <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, 
            <a href="https://planche.me/">Benjamin Planche</a>,
            <strong>Meng Zheng</strong>, 
            Terrence Chen,
            M. Salman Asif,
            <a href="http://wuziyan.com/">Ziyan Wu</a>,
            <br>
            <em>AAAI</em>, 2024
            <p></p>
            <p>
              We introduce Disguise, a novel algorithm that seamlessly de-identifies facial images while ensuring the usability of the modified data. Unlike previous approaches, our solution is firmly grounded in the domains of differential privacy and ensemble-learning research. Our method involves extracting and substituting depicted identities with synthetic ones, generated using variational mechanisms to maximize obfuscation and non-invertibility. Additionally, we leverage supervision from a mixture-of-experts to disentangle and preserve other utility attributes.
            </p>
          </td>
        </tr>	


        <tr onmouseout="PBADet_ICLR24_stop()" onmouseover="PBADet_ICLR24_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img class="image1" src="images/PBA_ICLR24.png" width="190" />
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
	    <a href="https://iclr.cc/virtual/2024/poster/17777">
          	<papertitle>PBADet: A One-Stage Anchor-Free Approach for Part-Body Association</papertitle>
	    </a>
            <br>
            <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>,
            Huayi Zhou, 
            Abhishek Sharma,
            <strong>Meng Zheng</strong>, 
            <a href="https://planche.me/">Benjamin Planche</a>,
            Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>ICLR</em>, 2024
            <p></p>
            <p>
              This paper presents PBADet, a novel one-stage, anchor-free approach for part-body association detection. Building upon the anchor-free object representation across multi-scale feature maps, we introduce a singular part-to-body center offset that effectively encapsulates the relationship between parts and their parent bodies. Our design is inherently versatile and capable of managing multiple parts-to-body associations without compromising on detection accuracy or robustness. Comprehensive experiments on various datasets underscore the efficacy of our approach, which not only outperforms existing state-of-the-art techniques but also offers a more streamlined and efficient solution to the part-body association challenge.
            </p>
          </td>
        </tr>	


	<tr onmouseout="CMDA_ICCV23_stop()" onmouseover="CMDA_ICCV23_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src="images/CMDA_ICCV23.png" width="190" />
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
	    <a href="https://arxiv.org/pdf/2307.15942.pdf">
          	<papertitle>CMDA: Cross-Modality Domain Adaptation for Nighttime Semantic Segmentation</papertitle>
	    </a>
            <br>
            Ruihao Xia, 
            Chaoqiang Zhao, 
            <strong>Meng Zheng</strong>, 
            <a href="http://wuziyan.com/">Ziyan Wu</a>,
	    Qiyu Sun,
	    Yang Tang
            <br>
            <em>ICCV</em>, 2023
            <p></p>
            <p>
             We propose a novel unsupervised Cross-Modality Domain Adaptation (CMDA) framework to leverage multi-modality (Images and Events) information for nighttime semantic segmentation, with only labels on daytime images. In CMDA, we design the Image Motion-Extractor to extract motion information and the Image Content-Extractor to extract content information from images, in order to bridge the gap between different modalities (Images &#60=&#62 Events) and domains (Day &#60=&#62 Night). Besides, we introduce the first image-event nighttime semantic segmentation dataset.
            </p>
          </td>
        </tr>	
        
	<tr onmouseout="multivHMR_stop()" onmouseover="multivHMR_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img class="image1" src="images/Multiv_HMR_AAAI23_teaser.png" width="190" />
              <br>  
              <br>  
              <img class="image2" src="images/Multiv_HMR_AAAI23.png" width="190"/>
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/abs/2212.05223">
              <papertitle>Progressive Multi-view Human Mesh Recovery with Self-Supervision</papertitle>
            </a>
            <br>
            <a href="https://gong-xuan.github.io/">Xuan Gong</a>, 
            <a href="https://lsongx.github.io/">Liangcheng Song</a>, 
            <strong>Meng Zheng</strong>, 
            <a href="https://planche.me/">Benjamin Planche</a>, 
            Terrence Chen, Junsong Yuan, David Doermann,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>AAAI (<strong>oral</strong>)</em>, 2023
            <p></p>
            <p>
              A novel simulation-based training pipeline for multi-view human mesh recovery, which (a) relies on intermediate 2D representations which are more robust to synthetic-to-real domain gap; (b) leverages learnable calibration and triangulation to adapt to more diversified camera setups; and (c) progressively aggregates multi-view information in a canonical 3D space to remove ambiguities in 2D representations.
            </p>
          </td>
        </tr>		

        <tr onmouseout="DPatientM_stop()" onmouseover="DPatientM_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/3DPatientM_MICCAI22.png' width="190">
              <br>
              <img src='images/3DPatientM_MICCAI22_kp.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://link.springer.com/chapter/10.1007/978-3-031-16449-1_12">
              <papertitle>Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion</papertitle>
            </a>
            <br>
            <strong>Meng Zheng</strong>, 
            <a href="https://planche.me/">Benjamin Planche</a>, 
            <a href="https://gong-xuan.github.io/">Xuan Gong</a>, 
            <a href="https://orcid.org/0000-0003-1535-447X">Fan Yang</a>, 
            Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>MICCAI (<strong>early accept</strong>)</em>, 2022
            <p></p>
            <p>
              A generic modularized 3D patient modeling method consists of (a) a multi-modal keypoint detection module with attentive fusion for 2D patient joint localization, to learn complementary cross-modality patient body information, leading to improved keypoint localization robustness and generalizability in a wide variety of imaging and clinical scenarios; and (b) a self-supervised 3D mesh regression module which does not require expensive 3D mesh parameter annotations to train, bringing immediate cost benefits for clinical deployment.
            </p>
          </td>
        </tr>	

        <tr onmouseout="ASN_stop()" onmouseover="ASN_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/ASN_IJCAI22.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://www.ijcai.org/proceedings/2022/241">
              <papertitle>Visual Similarity Attention</papertitle>
            </a>
            <br>
            <strong>Meng Zheng</strong>, 
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>, 
            Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>,
            <a href="https://sites.ecse.rpi.edu/~rjradke/">Richard J. Radke</a>
            <br>
            <em>IJCAI</em>, 2022
            <p></p>
            <p>
              A novel method to generate generic visual similarity explanations with gradient-based attention. It is agnostic to the specific similarity model type, e.g., we show applicability to Siamese, triplet, and quadruplet models. Furthermore, we make our proposed similarity attention a principled part of the learning process, resulting in a new paradigm for learning similarity functions. We demonstrate that our learning mechanism results in more generalizable, as well as explainable, similarity models.
            </p>
          </td>
        </tr>	

        <tr onmouseout="FL_TMI23_stop()" onmouseover="FL_TMI23_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/FL_TMI23.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/abs/2210.08464">
              <papertitle>Federated Learning with Privacy-Preserving Ensemble Attention Distillation</papertitle>
            </a>
            <br>
            <a href="https://gong-xuan.github.io/">Xuan Gong</a>, 
            <a href="https://lsongx.github.io/">Liangcheng Song</a>, 
            Rishi Vedula, Abhishek Sharma, <strong>Meng Zheng</strong>, 
            <a href="https://planche.me/">Benjamin Planche</a>,
            Arun Innanje, Terrence Chen, Junsong Yuan, David Doermann,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>IEEE Transactions on Medical Imaging</em>, 2023
            <p></p>
            <p>
              A privacy-preserving FL framework leveraging unlabeled public data for one-way offline knowledge distillation. The central model is learned from local knowledge via ensemble attention distillation. Our technique uses decentralized and heterogeneous local data like existing FL approaches, but more importantly, it significantly reduces the risk of privacy leakage.
            </p>
          </td>
        </tr>	

        <tr onmouseout="CRAeccv_stop()" onmouseover="CRAeccv_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/CRA_ECCV22.png' width="190">
              <br>
              <img src='images/CRA_ECCV22_pp.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/abs/2209.04596">
              <papertitle>Self-supervised Human Mesh Recovery with Cross-Representation Alignment</papertitle>
            </a>
            <br>
            <a href="https://gong-xuan.github.io/">Xuan Gong</a>,
            <strong>Meng Zheng</strong>,
            <a href="https://planche.me/">Benjamin Planche</a>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>, 
            Terrence Chen, David Doermann,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>ECCV</em>, 2022
            <p></p>
            <p>
              A Cross-representation alignment technique utilizing the complementary information from the robust but sparse representation (2D keypoints) for self-supervised human mesh recovery. Specifically, the alignment errors between initial mesh estimation and both 2D representations are forwarded into regressor and dynamically corrected in the following mesh regression. This adaptive cross-representation alignment explicitly learns from the deviations and captures complementary information: robustness from sparse representation and richness from dense representation.
            </p>
          </td>
        </tr>	

        <tr onmouseout="Pseudoeccv_stop()" onmouseover="Pseudoeccv_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/PseudoClick_ECCV22.png' width="190">
	      <img src='images/PseudoClick_ECCV22_pp.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660717.pdf">
              <papertitle>PseudoClick: Interactive Image Segmentation with Click Imitation</papertitle>
            </a>
            <br>
            <a href="https://sites.google.com/cs.unc.edu/qinliu/home?pli=1">Qin Liu</a>,
            <strong>Meng Zheng</strong>,
            <a href="https://planche.me/">Benjamin Planche</a>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>, 
            Terrence Chen, Marc Niethammer,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>ECCV</em>, 2022
            <p></p>
            <p>
              A generic framework that enables existing segmentation networks to propose candidate next clicks. These automatically generated clicks, termed pseudo clicks in this work, serve as an imitation of human clicks to refine the segmentation mask. We build PseudoClick on existing segmentation backbones and show how the click prediction mechanism leads to improved performance.
            </p>
          </td>
        </tr>	

        <tr onmouseout="PREFeccv_stop()" onmouseover="PREFeccv_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/PREF_ECCV22.png' width="190">
              <br>
              <img src='images/PREF_ECCV22_pp.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136820643.pdf">
              <papertitle>PREF: Predictability Regularized Neural Motion Fields</papertitle>
            </a>
            <br>
            <a href="https://lsongx.github.io/">Liangcheng Song</a>,
            <a href="https://gong-xuan.github.io/">Xuan Gong</a>, 
            <a href="https://planche.me/">Benjamin Planche</a>,
            <strong>Meng Zheng</strong>,
            David Doermann, Junsong Yuan, Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>ECCV (<strong>oral</strong>)</em>, 2022
            <p></p>
            <p>
              A predictability regularization by first conditioning the estimated motion on latent embeddings, then by adopting a predictor network to enforce predictability on the embeddings. We propose to regularize the estimated motion to be predictable. If the motion from previous frames is known, then the motion in the near future should be predictable. 
            </p>
          </td>
        </tr>	

        <tr onmouseout="SMPLA_stop()" onmouseover="SMPLA_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/SMPLA_CVPR22.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_SMPL-A_Modeling_Person-Specific_Deformable_Anatomy_CVPR_2022_paper.pdf">
              <papertitle>SMPL-A: Modeling Person-Specific Deformable Anatomy</papertitle>
            </a>
            <br>
            <a href="https://lsongx.github.io/">Hengtao Guo</a>,
            <a href="https://planche.me/">Benjamin Planche</a>,
            <strong>Meng Zheng</strong>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>, 
            Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>CVPR</em>, 2022
            <p></p>
            <p>
              A novel learning-based approach to estimate the patient's internal organ deformation for arbitrary human poses in order to assist with radiotherapy and similar medical protocols. The underlying method first leverages medical scans to learn a patient-specific representation that potentially encodes the organ's shape and elastic properties. During inference, given the patient's current body pose information and the organ's representation extracted from previous medical scans, our method can estimate their current organ deformation to offer guidance to clinicians.
            </p>
          </td>
        </tr>	

        <tr onmouseout="VOS_WACV22_stop()" onmouseover="VOS_WACV22_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/VOS_WACV22_teaser.png' width="190">
              <br>
              <img src='images/VOS_WACV22.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Yang_Multi-Motion_and_Appearance_Self-Supervised_Moving_Object_Detection_WACV_2022_paper.pdf">
              <papertitle>Multi-motion and Appearance Self-Supervised Moving Object Detection</papertitle>
            </a>
            <br>
            <a href="https://orcid.org/0000-0003-1535-447X">Fan Yang</a>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
            <strong>Meng Zheng</strong>,
            Terrence Chen, Haibin Ling,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>WACV</em>, 2022
            <p></p>
            <p>
              A Multimotion and Appearance Self-supervised Network (MASNet) to introduce multi-scale motion information and appearance information of scene for MOD. To encode multi-scale motion and appearance, in MASNet we respectively design a multi-branch flow encoding module and an image inpainter module.
            </p>
          </td>
        </tr>	


        <tr onmouseout="HMR_BMVC21_stop()" onmouseover="HMR_BMVC21_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/HMR_BMVC21_pp.png' width="190">
              <!-- <img src='images/HMR_BMVC21_pp.png' width="160"> -->
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://www.bmvc2021-virtualconference.com/assets/papers/0487.pdf">
              <papertitle>Everybody Is Unique: Towards Unbiased Human Mesh Recovery</papertitle>
            </a>
            <br>
            <a href="https://liren2515.github.io/page/">Ren Li</a>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
            <strong>Meng Zheng</strong>,
            Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>BMVC (<strong>oral</strong>)</em>, 2021
            <p></p>
            <p>
              A solution to the problem of obese human mesh recovery, i.e., fitting a parametric human mesh to images of obese people. We present a simple baseline that is scalable and can be easily used in conjunction with existing algorithms to improve their performance. We also present a generalized human mesh optimization algorithm that substantially improves the performance of existing methods on both obese person images as well as community-standard benchmark datasets.
            </p>
          </td>
        </tr>	

        <tr onmouseout="STRF_ICCV21_stop()" onmouseover="STRF_ICCV21_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/STRF_ICCV21.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Aich_Spatio-Temporal_Representation_Factorization_for_Video-Based_Person_Re-Identification_ICCV_2021_paper.pdf">
              <papertitle>Spatio-Temporal Representation Factorization for Video-based Person Re-Identification</papertitle>
            </a>
            <br>
            <a href="https://abhishekaich27.github.io/">Abhishek Aich</a>,
            <strong>Meng Zheng</strong>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
            Amit K. Roy-Chowdhury, Terrence Chen,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>ICCV</em>, 2021
            <p></p>
            <p>
              A flexible new computational unit that can be used in conjunction with most existing 3D convolutional neural network architectures for re-ID. The key innovations of the proposed STRF over prior work include explicit pathways for learning discriminative temporal and spatial features, with each component further factorized to capture complementary person-specific appearance and motion information. 
            </p>
          </td>
        </tr>	

        <tr onmouseout="VRX_CVPR21_stop()" onmouseover="VRX_CVPR21_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/VRX_CVPR21.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_A_Peek_Into_the_Reasoning_of_Neural_Networks_Interpreting_With_CVPR_2021_paper.pdf">
              <papertitle>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</papertitle>
            </a>
            <br>
            <a href="https://gyhandy.github.io/">Yunhao Ge</a>,
            Yao Xiao, Zhi Xu,
            <strong>Meng Zheng</strong>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
            Terrence Chen, Laurent Itti,
            <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>CVPR</em>, 2021
            <p></p>
            <p>
              A framework (VRX) to interpret classification NNs with intuitive structural visual concepts. Given a trained classification model, VRX extracts relevant class-specific visual concepts and organizes them using structural concept graphs (SCG) based on pairwise concept relationships. By means of knowledge distillation, we show VRX can take a step towards mimicking the reasoning process of NNs and provide logical, concept-level explanations for final model decisions.
            </p>
          </td>
        </tr>	

        <tr onmouseout="EVAE_CVPR20_stop()" onmouseover="EVAE_CVPR20_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/EVAE_CVPR20.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Towards_Visually_Explaining_Variational_Autoencoders_CVPR_2020_paper.pdf">
              <papertitle>Towards Visually Explaining Variational Autoencoders</papertitle>
            </a>
            <br>
            <a href="https://scholar.google.com/citations?user=6yh4f4YAAAAJ&hl=en">Wenqian Liu*</a>,
            <a href="http://bragilee.github.io/">Runze Li*</a>,
            <strong>Meng Zheng</strong>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
            <a href="http://wuziyan.com/">Ziyan Wu</a>,
            Bir Bhanu, <a href="https://sites.ecse.rpi.edu/~rjradke/">Richard J. Radke</a>,
            Octavia Camps
            <br>
            <em>CVPR</em>, 2020
            <p></p>
            <p>
              A novel technique to visually explain VAEs by means of gradient-based attention. We present methods to generate visual attention from the learned latent space, and also demonstrate such attention explanations serve more than just explaining VAE predictions. We show how these attention maps can be used to localize anomalies in images, demonstrating state-of-the-art performance on the MVTec-AD dataset.
            </p>
          </td>
        </tr>	

        <tr onmouseout="CASN_CVPR19_stop()" onmouseover="CASN_CVPR19_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/CASN_CVPR19.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Re-Identification_With_Consistent_Attentive_Siamese_Networks_CVPR_2019_paper.pdf">
              <papertitle>Re-Identification with Consistent Attentive Siamese Networks</papertitle>
            </a>
            <br>
            <strong>Meng Zheng</strong>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
            <a href="http://wuziyan.com/">Ziyan Wu</a>,
            <a href="https://sites.ecse.rpi.edu/~rjradke/">Richard J. Radke</a>
            <br>
            <em>CVPR</em>, 2019
            <p></p>
            <p>
              A framework for re-id that provides mechanisms to make attention and attention consistency end-to-end trainable in a Siamese learning architecture, resulting in a technique for robust cross-view matching as well as explaining the reasoning for why the model predicts that the two images belong to the same person
            </p>
          </td>
        </tr>	

        <tr onmouseout="RPIfield_stop()" onmouseover="RPIfield_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/RPIfield_CVPRW18.png' width="190">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w36/Zheng_RPIfield_A_New_CVPR_2018_paper.pdf">
              <papertitle>RPIfield: A New Dataset for Temporally Evaluating Person Re-identification</papertitle>
            </a>
            <br>
            <strong>Meng Zheng</strong>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
            <a href="https://sites.ecse.rpi.edu/~rjradke/">Richard J. Radke</a>
            <br>
            <em>CVPR Workshop</em>, 2018
            <p></p>
            <p>
              A new multi-shot re-id dataset, called RPIfield, which provides explicit time-stamp information for each candidate. The RPIfield dataset is comprised of 12 outdoor camera videos, with 112 known actors walking along pre-specified paths among about 4000 distractors. Each actor in RPIfield has multiple reappearances in one or more camera views, which allows the study of re-id algorithms in a more general context, especially with respect to temporal aspects.
            </p>
          </td>
        </tr>	

        <tr onmouseout="TBIOM20_stop()" onmouseover="TBIOM20_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/TBIOM_20.png' width="180">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://ieeexplore.ieee.org/document/9186207">
              <papertitle>Towards Automated Spatio-Temporal Trajectory Recovery in Wide-Area Camera Networks</papertitle>
            </a>
            <br>
            <strong>Meng Zheng</strong>,
            <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
            <a href="https://sites.ecse.rpi.edu/~rjradke/">Richard J. Radke</a>
            <br>
            <em>IEEE Transactions on Biometrics, Behavior, and Identity Science</em>, 2020
            <p></p>
            <p>
              A new algorithm to automatically reconstruct the time-stamped spatial trajectory of a person of interest moving in a camera network. With this output, a surveillance system user can easily tell where in the camera network the person of interest was located at any specific time.
            </p>
          </td>
        </tr>	

        <tr onmouseout="SPM18_stop()" onmouseover="SPM18_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/SPM18.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://ieeexplore.ieee.org/document/8253598">
              <papertitle>The Deep Regression Bayesian Network and Its Applications: Probabilistic Deep Learning for Computer Vision</papertitle>
            </a>
            <br>
            <a href="https://scholar.google.com/citations?user=PgbUI2sAAAAJ&hl=en">Siqi Nie</a>,
            <strong>Meng Zheng</strong>, Qiang Ji
            <br>
            <em>IEEE Signal Processing Magazine</em>, 2018
            <p></p>
            <p>
              A review of different structures of deep directed generative models and the learning and inference algorithms associated with the structures. We focus on a specific structure that consists of layers of Bayesian networks (BNs) due to the property of capturing inherent and rich dependencies among latent variables.
            </p>
          </td>
        </tr>	

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Review Activities</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

        <tr>
          <td width="100%" valign="center">
            IEEE Signal Processing Letters
            <br>
            International Journal of Computer Vision
            <br>
            IEEE Transactions on Neural Networks and Learning Systems
            <br>
            IEEE Transactions on Circuits and Systems for Video Technology
            <br>
            IEEE Access
            <br>
            European Conference on Computer Vision (ECCV)
            <br>
            IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
            <br>
            IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
            <br>    
	          International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)
            <br>  
	          International Conference on Computer Vision (ICCV)
            <br>
	          Annual Conference on Neural Information Processing Systems (NeurIPS)
            <br>
          </td>
        </tr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

        <tr>
          <td width="100%" valign="center">
            <a href="https://alert.northeastern.edu/news-article/from-phd-candidates-to-professionals-alert-trio-continues-to-collaborate/">Interesting and exciting media post about my advisor and collaborators!</a>
            <br>         
          </td>
        </tr>


        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:x-small;">
                Thanks to Jon Barron for sharing <a href="https://github.com/jonbarron/jonbarron_website">source code</a> for creating this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
